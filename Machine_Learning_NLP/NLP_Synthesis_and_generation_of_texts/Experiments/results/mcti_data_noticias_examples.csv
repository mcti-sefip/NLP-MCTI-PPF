method,summary,golden,source
SumyRandom,"Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). https://gerardmaggiolino. ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumyRandom,"If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. Ted Chiang: Yes, yes, and again, it's always the element of surprise. It is not simply that the program performs better than you expected on a certain test. ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumyRandom,"GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumyRandom,"What sort of difference is there between that and novelty search? And then again, you want to cluster them and instantiate through them. Mayalen Etcheverry: That's a good question. Is there some kind of objective measure? ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumyRandom,"And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I think that intelligence and cognition can be made with many different substrates. No individual cell knows what a finger is, but the network can know. Would it be through minimization or through some other metric or theme that you would suggest people to focus on? ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
SumyKL,"Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. Getting Started with Gym. ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumyKL,"If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. It is that it's doing things which never occurred to you that it might do. ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumyKL,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumyKL,"So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Because at the end, we as human, we are evaluating discoveries. Mayalen Etcheverry: Yeah, that's it. ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumyKL,"But you know, that's not - I don't think that's because it has to be this way. The individual cells are going to do things. So in your system, you have to have the ability of lower level subunits. All of that is in the eye of the beholder. ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
SumyReduction,"An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumyReduction,"You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumyReduction,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumyReduction,"One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumyReduction,"So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
SumyLuhn,"Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumyLuhn,"And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumyLuhn,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumyLuhn,"But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumyLuhn,"We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
SumyLsa,"He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumyLsa,"Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumyLsa,"Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. It's a long-term investment into our future, and yet its funding is often underserved. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumyLsa,"Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumyLsa,"If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
SumyLexRank,"Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumyLexRank,"You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. Ted Chiang: That's an interesting idea; I'll have to think about that. What can we learn from that as AI developers? That would, I think, be a really interesting and momentous development in AI. ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumyLexRank,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumyLexRank,"This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumyLexRank,"But you know, that's not - I don't think that's because it has to be this way. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. And if we look at them and communicate with them in the right way, they can do a lot of work for us. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
SumySumBasic,"GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. A well-known example of CA is the Game of Life. gym. ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumySumBasic,"You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? Ted Chiang: That's an interesting idea; I'll have to think about that. What can we learn from that as AI developers? ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumySumBasic,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumySumBasic,"At least in a system like Lenia, but in a given state space. And that was the point of this research experiment. So that's how we formulated metadiversity. Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumySumBasic,"And what do you do? Obviously, I don't have a final answer. It seems to know. All of that is in the eye of the beholder. ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
SumyTextRank,"An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). ",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
SumyTextRank,"You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. ",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
SumyTextRank,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
SumyTextRank,"One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. ",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
SumyTextRank,"So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. ",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
Transformers-facebook/bart-large-cnn,Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI,Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
Transformers-facebook/bart-large-cnn,Ted Chiang is an acclaimed author whose reach transcends the science fiction world. Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus,A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
Transformers-facebook/bart-large-cnn,"GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
Transformers-facebook/bart-large-cnn,"Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. She is part of a long-term research program working on autonomous developmental learning. Her talk at the ICLR 2022 workshop will address how met",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
Transformers-facebook/bart-large-cnn,"Michael Levin is a Distinguished Professor at the Biology department of Tufts University. His research aims to harness the bioelectric dynamics towards rational control of growth and form. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
Transformers-google/pegasus-xsum,Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA),Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
Transformers-google/pegasus-xsum,"We're not close to an AI, which is human-like at the moment, according to one of the world's leading experts on artificial intelligence and science fiction, who will give a talk at the 2022 ICLR workshop on collective learning.",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
Transformers-google/pegasus-xsum,GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Bal and Martin Venhart. GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Bal and Martin Venhart.,GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
Transformers-google/pegasus-xsum,"Mayalen Etcheverry talks to Nicholas Guttenberg about developing curiosity-driven models aimed at enabling agents to generate their own self-learning curricula. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
Transformers-google/pegasus-xsum,"In our series of interviews with scientists at the Institute for Computational and Life Sciences (ICLR) at the Massachusetts Institute of Technology (MIT), we speak to Michael Levin, who is giving a talk at the ICLR workshop on collective learning, the wisdom of the body",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
Transformers-csebuetnlp/mT5_multilingual_XLSum,"The BBC's weekly The Boss series profiles different researchers from around the world. This week we speak to Sebastian Risi, a Danish scientist who developed 3D structures of varying complexity.",Policy Learning with Neural Cellular Automata,"Sebastian Risi, IT University of Copenhagen. Summary Sebastian Risi of IT University of Copenhagen receives a GoodAI grant for his work with neural cellular automata (NCA) to grow neural network policies. Open-ended search methods that will be developed in this project are aligned with methods desirable in GoodAI's Badger architecture. Risi and GoodAI share the important research goal of scaling to more complex tasks. GoodAI's Grants program has awarded Sebastian Risi with funding for his research project with Neural Cellular Automata (NCA). An extension of previous work with NCA to generate 3D structures of varying complexity (Sudhakaran et al. , 2021), Risi now sets out to apply the algorithm to grow neural network policies that will adapt to different circumstances in complex reinforcement learning (RL) tasks and in a self-organized way. Once a policy is developed for the single cell (the update rule), adaptation during the agent's lifetime will not rely on the more general (but slower) RL algorithms or evolutionary algorithms (EAs), instead learning by itself through the update rule. Neural cellular automata will be trained to evolve and represent policies (behaviors) instead of rigid structures or bodies. These policies should grow to adapt to (= learn to solve) an arbitrary given task, without the need to encounter the task in the NCA training phase. Neural Networks as CA Rules Complex computational systems, cellular automata (CA) are governed by a few simple rules. Used to understand how complexity can emerge from a network of identical nodes, cellular automata serve as test beds of more advanced models. A well-known example of CA is the Game of Life. Invented by British mathematician James Conway in the 1970's, the Game of Life is a zero-player game. Its evolution is determined by its initial state without input from human players. One interacts with the game by creating an initial configuration and observing how it evolves. CA whose rules are represented by neural networks can be seen to implement ontogenetic dynamics analogous to the developmental process of living organisms. Starting from a single cell, they develop complex functional bodies with specialized organs without the need to explicitly encode all the information in the genome. Risi proposes to train the parameters of the NCA with evolutionary algorithms and gradient descent-based approaches. He points out that optimizing an NCA to grow a neural network can be a very deceptive search space, even as early results have been promising with the NCA agent being able to solve several OpenAI gym (Getting Started with Gym, n. d. ) and pyBullet (Maggiolino, 2019) tasks. One of the grant goals is to evaluate the learned learning policies on continual learning tasks. Scalability and open-ended searches The NCA approach fits very well into GoodAI's Badger architecture. Like Badger agents, every cell within the growing neural network consists of sub-agents with a shared policy that only communicate locally. Complementing the CA approach where agents connect on a 2D grid, the agents in Risi's system will operate directly on a neural network graph representation, trying to learn both how to grow a policy neural network and adapt it. Illustration of a 'Badger' agent. A single agent comprises a number of experts (blue/pink circle) that operate according to the same fixed and shared policy (blue circle). Each expert has its own unique internal state (pink circle). By changing the seed from which the neural network grows, there's potential to grow neural networks that perform different tasks, all from the same NCA. Integrating this ability within the Badger architecture could bring new directions for continual and lifelong learning, as well as aid the system to scale to more complex tasks. Elements of active research such as open-ended search methods, self-play, and auto curricula are critical for Risi's system to develop NCAs that generalize well. These same problems are key for the development of Badger architecture and are a marker that this grant project is a good match for GoodAI's Badger architecture research. To date, most of what we consider general AI research is done in academia and inside big corporations. We believe that humanity's ultimate frontier, creation of general AI, calls for a novel research paradigm, to fit a mission-driven moonshot endeavor. GoodAI Grants is part of our effort to combine the best of both cultures, academic rigor and fast-paced innovation. We aim to create the right conditions to collaborate and cooperate across boundaries. Our goal is to accelerate the progress towards general AI in a safe manner by putting emphasis on community-driven research, which in the future might play a key role in preventing the monopolization of AI technology (see AI race). If you are interested in Badger Architecture and the work GoodAI does and would like to collaborate, check out our GoodAI Grants opportunities or our Jobs page for open positions! For the latest from our blog sign up for our newsletter. References Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro, Claire Glanois, Sebastian Risi ""Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"". Proceedings of the 2021 Conference on Artificial Life (ALIFE 2021) Openai. (n. d. ). Getting Started with Gym. gym. openai. https://gym. openai. com/docs/ Maggiolino, G. 2019, October 22. Creating OpenAI Gym Environments with PyBullet Part 1. gerardmaggiolino. medium. com. https://gerardmaggiolino. medium. com/creating-openai-gym-environments-with-pybullet-part-1-13895a622b24"
Transformers-csebuetnlp/mT5_multilingual_XLSum,"Scientist Ted Chiang explains why artificial intelligence (AI) is human-like at the moment, and how it can be handled by society and individuals, at a conference in London.",A Conversation with Ted Chiang,"Ted Chiang shares his thoughts with GoodAI's Olga Afanasjeva on the limits of framing learning in terms of optimization and how we can draw inspiration from biological models of adaptation. An acclaimed author whose reach transcends the science fiction world, Chiang's prose blends a broad view of science and technology with a deeper philosophical inquiry into the human condition. His upcoming talk at the 2022 ICLR workshop on collective learning will focus on his novella, The Lifecycle of Software Objects, a narrative tracing the lives of Ana and Derek as they care for primitive artificial intelligences called digients. At its heart, the tale speaks to the complex relationship between society, individuals, and emerging technology. Poignant and compelling, the story raises important questions around responsibility, consent, and choices on the path of AI development. This interview has been edited for clarity. Transcript: Olga Afanasjeva: Why do you think that we're not close to an AI, which is human-like at the moment? If you can expand a little bit on the problem of optimization and the fixation of society as a whole on optimization. You refer to it in some of your interviews - what is the problem with optimization? Maybe not just in society on a whole, but in AI development? How do you see it? Ted Chiang: There are a couple of informal laws that people have coined: one is Goodhart's Law, which states that once a measure becomes a target, it ceases to be a good measure. And there are other laws which express similar sentiments, like Campbell's Law, which states that once you use quantitative social indicators for decision making, it becomes subject to your corruption pressures and it becomes a poor indicator of social processes. One commonly discussed example of this phenomenon is standardized testing and how it's intended to measure how good of an education a school provides, but it loses its usefulness when it becomes possible for teachers to teach to the test. I think there's this analogy to a lot of what has happened in the history of AI. A lot of AI programs are essentially standardized-test-taking machines; their entire development was a form of teaching to the test. In the history of AI, people have often said that people keep moving the goalposts for what qualifies as AI, that as soon as AI solves a problem, critics say, well, that wasn't really AI, real AI means doing this other thing. I don't think that's moving the goalposts. I'd say a more accurate way to think about it is that people are recognizing the ways in which AI programmers have been teaching to the test. People initially thought a certain task would be a good way to measure an AI's ability and proposed it as a standardized test, but then AI programmers found a way to teach to the test. And so in this sense, we aren't moving the goalposts; we are just trying to identify a test that the programmers have not already studied extensively. When people criticize the role of standardized testing in education, they're not saying that tests are useless or that tests have no inherent value. What they're saying is that our current tests are too easy to game. A lot of AI research is built around teaching to the test - whenever you define an objective function, whenever you define a loss function, you are basically establishing a single, extremely well-specified test, and you are building a machine that will score high on that test. This insistence on optimization is a misguided focus on a test. The question is, how do you actually gauge whether a school is providing a good education? You need some sort of testing, but it has to be a test that is hard to teach to, and one of the ways that you might do that is to create a test which is very unpredictable. What would that look like in AI? The researcher Francois Chollet has proposed something he calls the ARC, the Abstraction and Reasoning Corpus. It's a test where the developers know the format of the test, because they have to be able to code their AI to understand the inputs and deliver an output, but the developers have no access to the questions that the AI will be tested on. And that's a really interesting idea; it seems like a type of test which traditional optimization techniques are not applicable to. I think it's going to be very hard to define an objective function because all the developers will ever get is a final score back; they won't know what the specific questions were that their AI either answered correctly or incorrectly. What they'll have to do is come up with some more general problem-solving mechanism and hope that it works. And developers will probably try to optimize around the format of the inputs and the outputs, so it would be interesting if a lot of people started offering tests of this sort with very different input and output formats, because that would force the developers to generalize what sorts of inputs their program could accept and what sort of outputs it could generate. If you had a lot of these tests, you might have a really good indicator of an AI's general problem-solving ability. You would have to, by analogy, provide your AI with a really good education, the kind we want schools to provide, and then hope they have the skills to accomplish whatever task is thrown at them. More broadly, there is the question of viewing things in terms of an objective function or a loss function, which leads to a kind of all-consuming or totalizing way of thinking. It can be very tempting to think of the world as just an optimization problem to be solved: if we could just define the appropriate objective function, we could solve everything in the world. I am super skeptical about that. I don't think that most aspects of life can be accurately characterized as an optimization problem. Right now, we as a society have already collectively arrived at a certain objective function, which is profit. A lot of people have internalized the idea that profit is the ultimate good: if something generates the maximum profit, that is by definition the best outcome. I think that's why AI and capitalism fit together really well - they share this underlying worldview. The goal of profit is so pervasive that it's hard for us to shake, and the people who resist that idea face an immense amount of pressure to conform. I'm not saying that everyone in AI is working to maximize profit, but they are following the same underlying worldview, the idea that once you have defined your objective function correctly, which often means pricing things appropriately, you will know how to obtain the best result. However, I would maintain that most of the outcomes that we want are not the result of maximizing an objective function. What is a good school? What is a good healthcare system? What is a good public transit system? What is a good society? What is a good life? The idea that these can be achieved by maximizing an objective function is not a healthy worldview. Olga Afanasjeva: Right. One thing I discussed with my colleague relating to that, about defining the goals as a function you can optimize, and let's say it's happiness - as humans, we have this adaptation to whatever is our best possible state. We feel happy about something, but after a while, it becomes the baseline, right? He said something that I really liked - that it's probably not about reaching the objective, but it's about moving on the gradient, that change of state from feeling miserable to feeling better. This is what's actually going to make us truly happy. What are your thoughts on that, maybe moving on the gradient, rather than optimizing or reaching the objective? Ted Chiang: That's an interesting idea; I'll have to think about that. It seems like it could be formulated as its own kind of objective function that you would then optimize for. It would be interesting because it would clearly not be maximizing anything like more conventional objective functions like profit; before long, you would have to abandon profit and then move in a different direction. So there would be this constant shifting of goals, or the quantity that you're trying to optimize. Would that make people happier? I think that's something that deserves experimental investigation. We could learn a lot just through empirical testing. Are people actually reliably happier over the long term if they keep seeking out this gradient, this shift? That is definitely an interesting idea that warrants further study. Olga Afanasjeva: Okay. Coming back to the objective function, what should be the question the developer isn't asking - well, if a developer isn't asking a specific question and doesn't know the specific question that AI is supposed to answer, then the test becomes meaningful, as Chollet says in his paper. If the developer doesn't know the question that AI should answer, doesn't know the goal, or presumably doesn't know all the future possible goals that we would want the AI to solve - what are the questions that we as developers need to ask ourselves when we develop AI? What kind of features do we want to seek in AI? Where would you start? Where would you look for inspiration? Ted Chiang: My personal preference has always been to look to biological models and the ways in which animals demonstrate intelligence. Animals are not like humans, but they are constantly solving problems, oftentimes problems which they have never encountered before. That sort of general problem-solving ability - even if it's not on the same level as human problem-solving ability - seems like a good place to start. There were recent experimental results published where they taught mice how to drive, did you see this? They put these mice in these little carts and inside each cart were these contacts, and when the mouse put its paws on them, the cart would go forward or turn left or right. The mice could see a food dispenser and tried to get their carts to go toward it, and the scientists found that the mice became quite good at driving. The mice were trained three times a week for eight weeks, and after that they were proficient, so that's twenty-four trials. Twenty-four trials and they learned a skill which no mouse has ever encountered before in the evolutionary history of the species. I thought that was really impressive. And more recently someone has apparently taught fish to do something similar; I was really surprised that fish are capable of that. Obviously these are not radically unfamiliar problems for animals, because they are still navigating physical space and seeking food as a reward, so it's not as if they're proving the Pythagorean theorem, but they were able to apply their general learning skills to a situation unlike anything seen in their natural environments. Do we have any program that could do anything like that, that could learn a new skill after twenty-four trials? Most AI programs need more like twenty-four million trials. Personally I would be much more impressed if AI researchers built a program that could learn a skill that the developers had never thought of within twenty-four trials. That would impress me more than AlphaGo or AlphaZero. I feel like it would involve a major breakthrough in our ability to implement a generalized learning mechanism. Olga Afanasjeva: Yes, for us, we look at it from the perspective of self-improving AI. What you just described, a lot of researchers will call it learning to learn. And I think that is basically a form of self-improvement that we find in humans and in human intelligence. We can learn how to learn better, we obviously have intrinsic capabilities, or innate capabilities that allow us to learn in the first place and we build new stuff on top of the stuff that we learned already. This makes us more powerful learners. It brings me to one of your thoughts about self-improvement, and correct me if my quote is not precise: that self-improvement isn't really possible on the level of a single individual. This is why we don't really have to worry about runaway doomsday scenarios in AI. Rather, it's a feature which is powerfully demonstrated in collectives, on the level of societal cultures. Can we talk a little bit about the mechanisms behind this powerful cumulative cultural learning that happens on the level of societies, on the level of civilizations that allows us as a humanity to self-improve. What can we learn from that as AI developers? Ted Chiang: I should say upfront that the whole topic of collective learning is not something that I am super well-versed in and is something that I'm hoping to learn more about by attending this workshop. It seems to me that the big advantage that humans have in collective learning is that we have language and that we are able to communicate to others what we have learned. Language isn't an absolute requirement because individuals can teach each other through demonstration, and we see this in social animals to some extent, but we haven't seen societies of animals ratchet upward over time in their capabilities. We've seen a few skills spread through a population, but the process doesn't continue indefinitely, and animals' lack of language may be a gating factor, because language is closely tied to the capacity for abstract thought. To what extent could we envision AI agents engaging in a similar form of collective learning? If you could design AI agents that were fully capable of communicating in language, I think you would definitely see collective learning. But short of that, if you had AI agents that were able to demonstrate things to each other in a way that the other agent would pick it up very rapidly - much more readily than they could by just messing around by themselves - I think that would be a setting suitable for collective learning. But it seems to me that those are very difficult tasks. This ties back into what I was talking about earlier about the unpredictability of tests. One of the things that characterizes human language is that you can express anything in language. Something similar is true with physical demonstration. You can't enumerate all the things that one person can demonstrate to another, any more than you can enumerate all the things that one person can express to another using language. I feel like the endless capacity for expression is a big part of what enables collective learning and the ever growing sophistication of culture among humans. The open-endedness of these communication methods is crucial. If you were trying to design AI agents that convey information to one another through some mechanism, it seems likely that they will have a much more limited communication medium, because that's the only thing we know how to implement. We don't know how to implement AI agents that generate novelty in their utterances. But I could imagine that in a simplified communication mechanism, you could create a sort of society of AI agents that did exhibit collective learning in the ways that we see in troops of monkeys, where one agent discovers the equivalent of, for example, washing potatoes in water to get the sand off and then teaching everyone else. That would be impressive. That would, I think, be a really interesting and momentous development in AI. Olga Afanasjeva: Yes. Especially if we can figure out how to make sure that this kind of cultural effect is cumulative. Another aspect that's interesting about collectives is this emergent aspect. So even when we were talking earlier about the goals, in a collective for example, there isn't one single entity that sets a goal for everyone to optimize. It depends on your beliefs, of course, but let's say the way we look at societies as an organism which produces different types of goals - sometimes they're conflicting, they influence one another - there's no need for someone to actually impose the goal, but still, meaningful goals can emerge. Silly goals can emerge as well obviously, but somehow we end up also making progress along the way. So this is interesting, the emergent property of collectives and what's at its core. Ted Chiang: Yes, yes, and again, it's always the element of surprise. The behavior which no one expected or predicted, or was trying to achieve, that is a really powerful indicator of the kind of intelligence that I think is really interesting. It is not simply that the program performs better than you expected on a certain test. It is that it's doing things which never occurred to you that it might do. Things which your prior experience - any test that you might have thought to devise - would not be applicable. We could see novelty like that, in say, a system of AI agents. These agents would not be necessarily useful or commercially viable or practical in any sense, not for a very long time. But a breakthrough like that would be really, really exciting. I think that would be much more interesting than the high performing but extremely predictable neural net achievements that we see talked about a lot these days. Olga Afanasjeva: I agree, Ted. Thank you so much. This is a beautiful note on which we can conclude. Thank you. Ted Chiang: Thanks for having me. Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: https://en. wikipedia. org/wiki/Ted_Chiang https://subterraneanpress. com/the-lifecycle-of-software-objects For the latest from our blog, sign up for our newsletter. "
Transformers-csebuetnlp/mT5_multilingual_XLSum,"Two Slovakian scientists have received a donation of more than €20,000 to help their research in the field of biology and physics, according to the BBC's weekly The Lancet.",GoodAI Supports Slovak Scientific Research,"GoodAI Research has provided funding to the ESET Foundation to support the research of scientists Miroslav Baláž and Martin Venhart. The donation is a contribution to the general advancement of science in Slovakia. GoodAI has allocated €20,000 from its grant program to the ESET Foundation, a nonprofit organization dedicated to education, research, and science in Slovakia. The funding is intended to support the endeavors of scientists Miroslav Baláž and Martin Venhart in their respective fields of biology and physics. Baláž and Venhart both studied and worked abroad for several years before returning to Slovakia to make contributions in their home country. Miroslav Baláž's research seeks to understand how metabolic activation of fat cells occurs and identify the molecular pathways and key genes that control the metabolic activity of brown adipose tissue. This natural physiological mechanism could be used to increase energy expenditure, particularly in obese patients. The acquired knowledge holds promise for new therapeutic options in the treatment and prevention of obesity, potentially leading to improved metabolic health, and offering insight into associated diseases. Martin Venhart and his team have been working on the structure of atomic nuclei and its properties to understand the phenomena of nuclei deformation. As he explains, this 'deformation' is one of the most basic properties of the atomic nucleus and a potential key factor in our very existence. A basic building block of the human body is carbon-12, whose atomic nucleus is very strongly deformed. Central to Venhart's research are new types of decay discovered in gold isotopes as well as the properties of radiation emitted by atomic nuclei. Mapping their behavior could bring knowledge for the field of nuclear medicine and the treatment of cancer. Funding for Baláž and Venhart's work comes from the GoodAI Grants initiative which, to date, has provided over $770,000 to researchers and research groups worldwide. In accordance with the program's ethos of community and collaboration, the donation departs from the standard Badger-focused research profile to acknowledge the value and role of science for the benefit of the public and society. ""Basic science needs support from the private sector as well as from the government. It's a long-term investment into our future, and yet its funding is often underserved. We want to send a message that this is important and we hope that others will follow,"" states founder Marek Rosa. For the latest from our blog, sign up for our newsletter. "
Transformers-csebuetnlp/mT5_multilingual_XLSum,"In our series of letters from African journalists, we speak to Mayalen Etcheverry, a French academic who is working on artificial intelligence (AI) research at the Institute of Large Research (ICLR).",A Conversation with Mayalen Etcheverry,"Discovering new forms of self-organized agencies. Example of identified pattern in Lenia, a generalisation of Conway's Game of Life (link to paper in references). Mayalen Etcheverry is a Machine Learning researcher at Inria, University of Bordeaux. A second year PhD candidate, she is part of a long-term research program working on autonomous developmental learning at the frontiers of AI and cognitive sciences. Together with her team, the FLOWERS group, they leverage ML techniques along with developmental psychology and neuroscience to find applications in robotics, human-computer interaction, automated discovery and educational technologies. Etcheverry speaks with GoodAI Research Scientist, Nicholas Guttenberg, about developing curiosity-driven models aimed at enabling agents to generate their own self-organizing learning curricula. Her upcoming talk at the ICLR 2022 workshop will address how metadiversity search, curriculum learning, and external guidance (environmental or preference-based) can be key ingredients for shaping the search process. This interview has been edited for clarity. Transcript: Nicholas Guttenberg: All right, well, welcome. This is an interview with Mayalen Etcheverry who is going to be giving an invited talk at the Cells to Societies' workshop at ICLR. Mayalen is a second year PhD candidate at Inria with the FLOWERS group, if I have that right. Mayalen Etcheverry: Yes. Nicholas Guttenberg: The topic of the research involves things such as curious AI, open-endedness, metadiversity search and automated goal generation, and intrinsic motivation. So I'm curious - you talk about the potential for open-endedness in your workshop abstract. Do you want to say what sort of things you intend by that or where you see that happening? Mayalen Etcheverry: Oh, yeah, sure. Well, first, thank you for inviting me to this workshop. When I'm talking of an open-ended system, in general, we mean a system whose behavior is not convergent over time. It's a system which keeps surprising you and producing new and interesting outcomes. From our computer's perspective, if we were able to design, come up with a computer program or an AI that would fit this description, well, I guess it would be very desirable, of course, across many practical domains. One sort of open-endedness that I'm really interested in in my work and for which I believe that AI can play a big role is with respect to our scientific discovery practice in complex systems. There are many complex systems that are studied by scientists at the bench - chemists are trying to discover new drugs or new materials. Biologists are trying to bio-engineer functional tissues or organs. Then you also have computer scientists, like my team, who are exploring complex numerical systems such as models of cellular automata - for instance, to inform about theories about the original life or intelligence. I'm talking about complex systems because I think that they are great candidates for open-endedness in the sense that they offer us unbounded possibilities for emergence. But at the same time, they're very hard to explore and navigate for us humans. I always take the example of the Game of Life as a good example. It's a very simple numerical system where we know all the rules. It's fully deterministic and it was created more than 50 years ago. But players are still discovering new and increasingly complex patterns in it - running it for hours, for instance, on supercomputers. It's a good example of how our discovery practice is really difficult in the system. I think that the same trend holds for chemical and synthetic biology systems. There is even this sort of trend or low, which is actually interesting. I think it's the reverse of Moore's law. Even though we have this exponential increase in technology, research, and computation - we are getting better and better at doing chemical tests and running them massively in parallel - but paradoxically, instead of any exponential increase, or making discovery much faster and cheaper, it's actually much slower and much harder. So that's, I think, a very interesting tendency. You could even say that as a society, we're starting to converge to this extreme. I'm not saying that scientific discovery is not open-ended. But I think that maybe we are reaching a point where the range of problems or scientific challenges that we are trying to tackle are so complex, that we really need new tools to help us tackle them to avoid this convergence point and to unlock new possibilities. So I think that that's what I mean when I talk about open-endedness - discovering the system, unlocking the possibility for discovery in complex systems. Nicholas Guttenberg: You had some recent work with Lenia where you were able to use this method to discover all sorts of behaviors that would be quite hard to hand-design. Mayalen Etcheverry: Exactly. We are working mainly on numerical systems so far, especially Lenia. In our latest work, we were able with machine learning tools to discover new forms of self-organized agencies - which really look like small life forms - that are moving around in the cellular automaton. And that's something that has been really hard to find by hand or by random exploration or by optimization methods. It's really not an easy problem. Nicholas Guttenberg: I wonder if I should show some of the some of the videos from your recent work - we can include a link. One of the things I'm curious about with regards to open-endedness is a lot of the examples you gave, you have a purpose in mind - like drug discovery - there's a demand on the human side of what we want that to find. But at the same time, for the system, there's something more innate to the system that it wants to discover, some way that it's going to have the most success following, or even things where it has to do something we wouldn't know as important in order to get to interesting states. Do you find any kind of tension between those things in your work? Or do you have any way of resolving that? Mayalen Etcheverry: Sure, so indeed, there's this problem that - especially when we are using machine learning programs - at the moment, we tend to strongly define the tasks that we want our system to be solving. So we are trying at the same time to get away from this too strong task specification that you do, typically, in optimization where you only define, for instance, a scalar reward and you want your system to converge to the peak in the fitness landscape. So we have been proposing - in previous works - some ways to escape these strong constraints. But at the same time, as you said, we don't want the system to go randomly and do things that are not interesting for us. Because at the end, we as human, we are evaluating discoveries. And so we want something that fits our preferences - it's not easy to manage balance where humans have preferences, but at the same time, we don't know how to define them and to compute a quantity out of it. And so we introduced a paper, for instance, this metadiversity paper. We have kind of this interactive thing where the agent should boldly explore the space, but at the same time show its discovery, for instance, periodically to a human, and then the human could give some feedback to guide back inspiration. Nicholas Guttenberg: So you mentioned, metadiversity - and there's flat diversity, equality diversity, where it's just trying to explore the space. But with this metadiversity idea, do you want to explain that? What sort of difference is there between that and novelty search? Mayalen Etcheverry: Yeah, sure. So this metadiversity idea, it's something that we came up with when we were trying in practice how to formulate exactly this problem of making an open-ended form of discovery assistance in complex systems. As you mentioned, the first idea or first family of machine learning algorithms that came to our mind was more like this novelty search type of algorithms - algorithms that come from evolutionary or developmental robotics. That seemed to be a good fit with respect to this optimization method because instead of trying to optimize the fitness, it would try to optimize the novelty of the discoveries. So we started with that, but then quite a critical part and well known critical part, is that they still assume - at least in their standard definition - that you have some sort of oracle representation. So that will basically, from your system's raw observation, describe the interesting degrees of behavioral variation in your outcome. So I like to see this representation as taking a lens from the system. It's like putting on a pair of glasses and only looking at those few factors of variation that can emerge in your system and then trying to find the most novelty and diversity within that lens. And so I guess a single lens makes sense, or were more successful for simpler physical systems like robotics. We have some experiments in the FLOWERS team where, for instance, you would put a torso robot with the arm that could be on a table and it could manipulate, for instance, a discrete set of objects in the room. And so here it makes sense to look at, instead of looking at the raw image, you could look at the position of the objects. And then if you would find novelty in this behavioral space, it means that your robot would learn to grasp objects and move them around. So it would make sense to look at this only through this lens of the system. But in the context of Lenia where we have this raw video of pixels, there is no one unique lens that you should use. There are tons of lenses that you can use to describe the system. And so it's not trivial first to define one lens and not trivial also to know the impact that using this lens will have on your flat novelty-search like discovery. To test that, we did this interesting experiment where we tried to run the same equivalent of a novelty search algorithm, but with different lenses. So one lens was hand-defined with statistics from the original Lenia paper, or we have one lens for which we would use Fourier descriptors. Then we also used unsupervised-learned lens in the sense that we pre-trained a VAE on a big database of Lenia patterns. And we even tried a VAE that was trained online so the lens would not be so static anymore, but it would be fixed in capacity. And so we ran the algorithm and we had two striking results coming out. If you ran the algorithm using lens A and you projected in the space of lens A, indeed, it's going to be very diverse for our set of final discoveries. So it shows that the novelty search is very efficient at finding new solutions. At least in a system like Lenia, but in a given state space. But if you take the same discoveries and project them through lens B, then they would achieve a very poor diversity because obviously, the variation that makes them diverse in space A will disappear. So here it will have missed potentially other types of interesting types of diversity. And that was the point of this research experiment. The metadiversity came very naturally as this idea was that your open-ended system should not try to find new solutions within its existing state or view of the system, but that it should continuously try to extend it and incorporate other degrees of variation that could emerge in the system. That was the intuitive idea behind the metadiversity. In this paper, we formulated it as a bi-level exploration loop where you have an agent in an outer loop who is trying to learn that divergent feature spaces and extend this module - a set of modules - feature space to characterize the system, which would be the lens. And then in an inner loop, we'll run some novelty search in each of those spaces. And also something that we emphasize that's very important, as you say, is to put some human in the loop to try to prioritize the state space that for the human would be interesting. So that's how we formulated metadiversity. Nicholas Guttenberg: So the relationship between the levels - it's like if I imagined, let's say, you just took every single pixel, you'd have this huge dimensional space. You would never really fill it. Or everything would be diverse automatically because everything would be different in some pixels and they wouldn't necessarily be meaningful differences. But when you construct the space, hierarchically, do you find that the kind of lenses you get are special in some way you wouldn't have been able to get if you tried to jump straight to them? Do the lenses themselves tell you something about the system? Mayalen Etcheverry: So first of all, you could say that why not train right away the huge dimensional space of pixels. But that's known to not be efficient for a novelty search type of algorithm. You need lower dimensional spaces. And then defended in this paper, we built them hierarchically. But that was one possible implementation of meta diversity search and I guess there are many other ways that you can do so. But the intuition about doing it hierarchically was that, first you don't know anything about the system. So you actually want some kind of VAE-like, some coarse compression of it that right away gives you the main factors of variation. Maybe the average intensity or the coarse form of the pattern or something like that. And then you probably want to cluster the discoveries, to start separating them into niches that seem to be more related between them. And then you want to instantiate a new lens per niche, so probably you will have this niche of texture looking patterns with very high frequencies and in another you won't have just the coarse description, but you want to go more in details and start seeing that you have some zebra patterns, or wave-like patterns. And then again, you want to cluster them and instantiate through them. You can have this coarse grain view that seems meaningful, but there are maybe other ways that's a good construct of lens progressively. Nicholas Guttenberg: So it's like each niche you're trying to find the thing about that niche that wants to vary, and then you just ask it to vary more or to vary completely? Mayalen Etcheverry: Yeah, that's it. Nicholas Guttenberg: Do you think that this resolves the lazy way that a lot of intrinsic motivated systems will just fill up entropy from the bottom? Because you're saying, here's the direction that you already want to vary and you can explore this, but I'm not going to give you every direction. It's going to be the top two directions at a time. And then when those are done, you have to find another split before you get to have another direction of entropy to fill up? Mayalen Etcheverry: That's a good question. So I guess it depends on the system. In Lenia, as I told you, we had very strong results that generating diversity in some direction was not driving diversity along other dimensions - for instance, we have a very striking example where we use this Fourier descriptor. Basically, we were characterizing frequencies in the image. At the end of the exploration, we would only have discoveries in Lenia that were purely vertical stripes. So that was interesting. The intrinsically-motivated system had exploited the fact that you're searching for diverse frequencies, but it would only show you vertical stripes. So indeed, with all kinds of frequencies, but intuitively it's not what you expected of diversity. Even if you have all kinds of frequencies, you wanted to have some other types, maybe wavy forms or localized patterns. So you could say some sort of lazy indeed problem for intrinsically-motivated system. And once again, it shows the importance of having good goal spaces, a good way to characterize the system. Nicholas Guttenberg: Along those lines, do you have some idea? Is there a standard by which you could say this is a good goal for a system to have, or this is a bad goal for a system to have? Is there some kind of objective measure? Or does it really have to be constructed from within the system's point of view? Mayalen Etcheverry: So what is a good goal once you're in this task space that you define? I guess so at least in the FLOWERS team where we're working a lot in this intrinsically-motivated goal setting system and we generally say that good goals should be around two dimensions. First, as we discussed, a good goal should be novel and interesting. So it should be this kind of creative goal. And the other dimension is that it should be feasible, it should be learnable. So it should not be too easy or too hard. Personally, in my work I've used very basic strategies for all of these dimensions. For novelty, I was simply uniformly sampling in the goal space, with the intuition that if you manage to uniformly cover the space then you should reach maximum diversity. Then for the interesting part, well, we use this basic scoring system where you would add some human that could score the state spaces that he is interested in, and so you would prioritize goal sampling in those spaces. And for the feasible part, we either didn't implement - I mean, I in my work, I didn't implement any smart thing. Or in our latest work that you mentioned at the beginning, we have this curriculum, basic curriculum idea where we sample goals not too far from the set of goals that we had already reached. But there are, for the three dimensions, many other and more advanced strategies that have been proposed, especially in the FLOWERS team. For novelty, you could have count-based estimation or you could train some generative model distribution and try to sample inversely proportional to the probability of sampling in the distribution. There are colleagues in my team that are trying to incorporate language in the goal sampling strategy such that a human could somehow communicate goals. That would be very cool. And for the curriculum, you have also more advanced strategies of automatic curriculum learning where, for instance, there is this idea of learning progress. So you can empirically estimate how good you're doing across different goal regions, and then you would sample toward the goals of intermediate difficulty that are not too easy or are not too hard. Nicholas Guttenberg: Oh, great. Well, thank you for your answers and for giving us the time to have this interview and I look forward to your talk. Mayalen Etcheverry: Thank you Nicholas. It was a pleasure. Nicholas Guttenberg: Thanks. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home References: http://developmentalsystems. org/intrinsically_motivated_discovery_of_diverse_patterns For the latest from our blog, sign up for our newsletter. "
Transformers-csebuetnlp/mT5_multilingual_XLSum,"In our series of letters from African journalists, the BBC's weekly The Boss series profiles different scientists from around the world. This week we speak to Michael  Levine, a professor at the Institute of Large Research (ICLR)",A Conversation with Michael Levin,"Cells Vectors by Vecteezy Michael Levin is a Distinguished Professor at the Biology department of Tufts University and serves as director of the Tufts Center for Regenerative and Developmental Biology and the Allen Discovery Center. He speaks with GoodAI Senior Research Scientist, Jan Feyereisl, about the philosophical foundations of his work, which include theories of cognition that assert the goal-seeking behavior associated with the ""self"" is apparent at all scales of life. A conceptual shift from the viewpoint of the brain as the cognitive-engine of the body, Levin proposes that every level, from molecular networks to cells, tissues, organs, organisms and swarms, each possess their own goals and agendas. The flexibility, plasticity, and robustness due to the multi-scale competency architecture of biological systems. Focused on the molecular mechanisms that cells use to communicate with one another in developing embryos, Levin's research aims to harness the bioelectric dynamics towards rational control of growth and form. The language medium: bioelectricity. One proof-of-concept for this view of the world, xenobots - synthetic life forms created in his lab from the skin and heart-muscle cells of frogs - demonstrate the ability of organisms to grow and regenerate through the careful direction of goal-seeking behavior. The topic of his upcoming talk at the ICLR workshop on collective learning, the wisdom of the body: evolutionary origins and implications of multi-scale intelligence, offers inspiration for new machine learning and robotics approaches. This interview has been edited for clarity. Transcript: Jan Feyereisl: Welcome, everyone. Welcome, Mike. Thank you very much for joining us. Just to introduce myself, my name is Jan Feyereisl. I'm a research scientist at GoodAI, a research lab based in Prague in the Czech Republic, focused on building artificial general intelligence systems. It was founded by Marek Rosa, who also founded a games company that built a game called Space Engineers, whose mantra is ""the need to create. "" Together with our friends and colleagues from Google research, we are organizing a workshop at ICLR this year called Collective Learning Across Scales, from Cells to Societies. This discussion is to get people interested in the workshop, in the speakers that will be participating in the workshop, and to let the audience, potential participants, and anyone else interested in those topics to kind of learn a little bit more about the topics and about the works of the invited speakers. One of them who is here with me today is Professor Michael Levin. Welcome. Michael Levin: Thank you so much. Happy to be here. Jan Feyereisl: Thank you. Let's start. So in some sense, I view your work - and apologies if I misrepresented, you can correct me if I'm wrong - but in some sense, I view it as looking at some form of fundamentals of how elementary biological units communicate among each other in order to give rise to some form of collective or group-wise learning behavior. You're doing it at a level that is really exciting to look at because it allows essentially to communicate with the biological systems in a high level language that allows you to do things that traditionally in biology were not possible. Instead of micromanaging, you can essentially focus on pinpointing high level structures or sub-routines that will do the work for you. And the way that I understand it, it relates to bioelectricity, in some sense, or in other words, the way that cells communicate among each other. So my first question relates to this bioelectricity. Would you say that there is some kind of a fundamental process in terms of this bioelectric communication that is shared across all cells in a living organism? And maybe from which neural communication - that in machine learning we really focus on maybe too much - originated? Michael Levin: Let's take one small step back. I just want to point out that you're absolutely right - in terms of our empirical work, that's what we do. We study how cells communicate with each other to scale up into larger kinds of systems. But more broadly, what I'm interested in is diverse embodiments of mind and intelligence. I want to understand how different configurations of physical objects can give rise to things that we recognize as intelligence, cognition, memory, learning, preferences, and so on. And so I think that what evolution has done is discovered long before we did, that electricity and electrical networks are a really convenient way of doing that. And I'll talk about the details of that but I think it's important to say that I don't believe in any kind of a privileged substrate for intelligence. I don't think that bioelectrics is somehow magical in the sense that that's the only way you can get intelligence. I don't think that neurons, brain synapses, that these kinds of things are uniquely, in some way, required for intelligence. I think that intelligence and cognition can be made with many different substrates. Now here on Earth, we have a few good examples. Most of them do, in fact, involve bioelectricity. But you know, that's not - I don't think that's because it has to be this way. I think there are some very wide spaces of possibilities for how to embody intelligence. And I think you're absolutely right in that what's important about the way that cellular collectives get together to have goals, preferences, all of these things that we recognize as cognition above the level of single cells. There's nothing really specifically neural about it. So most of the paradigms of, let's say, connectionist types of ideas in machine learning and so on - they're not really about neurons. I mean, every cell does the kinds of things that the network models are doing. So there's nothing actually very, very neural specific about it. And that's good, because it's not even easy to say what a neuron is. Neurons evolved from much more primitive cell types. Cells have been using electrical communication to form networks since the time of bacteria and bacterial biofilms. It's that old. Every cell does many of the things that in fact, most of the things that neurons do. Jan Feyereisl: Thank you. So, if you would then take it even a little bit further down, further away from biology, do you believe that there is some kind of indication that potentially there is some universal rule or mechanism that could describe cognition, intelligence across many more scales than just the biological ones going from physics all the way to societies and the universe as a whole? Michael Levin: Well, I can say this. We've been thinking pretty hard about a way to come up with some invariants, something that's going to be the same in all cognitive intelligence systems, regardless of their implementation, regardless of what they're made of, and regardless of their origin story, whether they're evolved, engineered, or some combination of the two. I think this is really important because in the past - due to the limitations of technology, and frankly, our imagination - in the past, it was easy to distinguish between machines and intelligent living organisms. People to this day are writing papers on how living things are not machines and so on. And the thing is that this was because in decades past, you could look at something and you could sort of knock on it and if you hear a metallic sound, you could conclude that, okay, this is a machine. It came out of a factory. It's going to be pretty boring. It's not going to have any of the features we associate with living things. Whereas if you touch it, and it's soft, and squishy, you say, okay, this is probably evolved, we owe it some ethical consideration, and it will do interesting things and so on. That distinction is completely artificial. It's not going to survive the next decades now because of evolutionary techniques used in engineering and because of chimeric technologies in bioengineering technologies. There is absolutely no firm line to be drawn between these things. So that means that going forward into the future, we really have to establish categories that are deep and not just because of the limitations of what happens to have come out of evolution, or we could engineer at the time. So to me the most profound invariant for all cognitive systems, is the ability to pursue goals. So what you can imagine is - and of course, I'm not the first person to say this - Wiener and Rosenblueth* had this nice paper in the late 40s, talking about the spectrum of goal-directed activity as a marker for cognition. So this is an old idea. But I've sort of formalized it more biologically in recent papers, talking about this kind of goal space for any system, whether it be evolved design, natural, artificial, alien, whatever it's going to be. You can imagine the spatio-temporal scale of the largest goal it can possibly pursue. So this kind of demarcates the kind of cognitive horizon beyond which the system cannot think. So if you're a bacterium, the only goals you can really pursue - they're very small in space and time - they're local. Let's say local sugar concentration. Maybe you have a few minutes of memory going back, maybe a little predictive power going forward. But that cone, that light cone is really quite small. Whereas if you're, let's say, a dog, then you might have some pretty good memory extending backwards. You have pretty good capacity going forwards. But your cognitive system is simply not going to be able to represent and care about states that are going to happen three months from now, 20 miles over, it's just not going to happen. And if you're a human, you have perhaps enormous scale goals, maybe even longer than the human lifespan, which leads to interesting psychological issues, right? We're the first creature that can actually conceive of goals that are guaranteed to be unachievable. Things that take longer than the human lifespan. So I think that that kind of strategy of trying to map out what the shape of the possible, the most, the biggest goal that a system is able to pursue, that's able to be stressed by if that goal isn't met, that gives you shapes in this virtual space of this virtual bowl space that allows us to compare across really diverse systems. It doesn't matter what they're made of, right? So at that point, you are now free to consider all kinds of unusual embodiments for agents. You can think of AIs. You can think of very small things like molecular networks. You can think of societies. You can think of gravitational synapses. You can come up with all sorts of interesting things. And all of them can be placed somewhere on this kind of diagram next to each other no matter what they're made of. Jan Feyereisl: So that's very interesting. You're saying that the goals really - and the representation of goals, and the space of all the possible goals related to that particular cognitive system, that particular intelligence - is the one that we could focus on in order to describe intelligent systems across scales? Michael Levin: That's correct. Jan Feyereisl: So maybe the next related question to that is related to where do goals come from. So I think just like in machine learning, or in AI, if you want to build agents that, in some sense, are open-ended, and they're able to develop themselves, or in some sense, evolve for a very long time ahead - how do we actually create such systems that are able to invent their own goals and continue to actually do something useful? Any suggestions on where to look for the origins of goals? Michael Levin: Yeah, this is a very profound question. And I think we have to be humble about the fact that we can only begin to sketch the answer. So I'm certainly not going to claim I have all the answers, but I can say a few things, how we think - thought about it. In biology, the common story is that goals, like everything else are - if they exist at all - according to the standard paradigm, shaped by evolution by selection. So there were some particular environments that shaped your ancestor's goals, and thus they shape your goals. And some of those goals are emergent in the sense that we have to understand that genetics doesn't specify the organism and it doesn't specify the behavior of the organism. It specifies the micro-level hardware that is available. And then the behavior of that hardware, in terms of physiology and behavior and learning and so on, is what gets selected upon. So that's the standard story -that they come from selection. But I think in many ways this is - this story is highly incomplete. And lots of people have said this before me. I'll just give you a recent example of this in our group. We have been working on this thing which we call xenobots*. You take an early frog embryo. And without adding anything to it - so no new genes, no nanomaterials, nothing like that - you simply remove, you take off some of the skin cells and you put them in a separate environment. And what you've done is you've taken away some of the constraints that normally tell those skin cells to have this very boring two dimensional life on the outside of the embryo keeping out the bacteria, and you allow them to sort of reboot their multicellularity. And you say, okay well, what do you want to be actually without the constraint of all these other cells? What do you want to be? And it turns out that these cells - there's many things they could have done. They could have separated and crawled away. They could have made a flat mono-layer, like a tissue coat, like a cell culture. They could have died - many things they could have done. Instead, what they do is they get together and they make this little creature that is motile. So it swims along. It's completely self-powered, self-motivated. It swims along and has all kinds of behaviors. It can regenerate. And one of the things that it also can do is work. If you are provided with loose cells in the medium, it basically does what we call kinematic self-replication. That's the kind of von Neumann style replication where they will go around, collect the cells into little piles, and those cells become the next generation of xenobots. And then they go and do the same thing. Now, what's important about that is where do the goals of the frog embryo come from? Well, for millions of years, they were selected by the need to be a good frog in a froggy environment and have high fitness and all of that. But now, there's never been a selection to be a good xenobot, there have never been any xenobots. And so what this means is that you take these cells and within 48 hours, they learn to solve this problem of being a creature with a new set of components, right? They don't have the typical things they would normally have. In particular, they don't have any way of reproducing the way that frogs normally reproduce. We've made that impossible. In 48 hours, they figured out a completely new way to get the job done that as far as I know, no other animal on earth does. What did evolution actually learn when making the frog genome? It wasn't just to make a good frog. That's clear. We don't know what exactly it learned. But what I think is clear is that evolution doesn't produce specific solutions to specific environmental problems. It produces problem solving machines that have - and I have some thoughts about what power is that ability - but it produces machines that can solve problems in other spaces, which leads to the very profound question that you asked me, where do the goals of a xenobot come from? I don't know, I think that it's clear that selection is not the entire story, maybe not even the main story. And I can sort of speculate on some things. But I think the most important thing to say is that it's a very profound question that is not explained by advances in genomics and things like that. Jan Feyereisl: Okay, that's good to hear that we have a lot of potential exploration and interesting research to be done in this area. It personally interests me and kind of relates to the work that we do at GoodAI as well as many other researchers. And in relation to machine learning and artificial intelligence, I think the really interesting question there is related to the ability of biological systems to essentially have somehow solved the issues of generalization and extrapolation. We actually are trying to build collective systems in our simulations, in our agents, and exactly as you said, rather than evolution, or in our case, our learning algorithm trying to find particular solutions to specific problems or tasks, we focus on building or searching for problem solving machines or learning algorithms themselves. But many times what happened was that those systems were too specific, they always in some sense ended up converging or ended up getting stuck or being too biased towards what they were trained on. So do you have any indication or understanding about how evolution was able to achieve the discovery of problem solving machines that allow you to essentially take something like skin cells, create a collective out of them, to work in completely different configurations? And in a probably relatively different environment than what they were evolved for? Michael Levin: Yeah, so I guess I can say two things. And of course this is very much an open question. But the first thing is that it's important to realize that the only key deliverable of evolution is making sure that its products are observable by some observer, like us. In other words, evolution doesn't necessarily make more intelligent things, or more complex things. All we can assume that's going to be the result of the process of biological evolution is biomass. It's going to produce something that sticks around long enough for us to see it, whether that be through survival, or through a long period of time, or reproduction. Or whatever it is, that's really all. And so evolution is interesting because if that's your only constraint - to be propagated long enough for somebody to observe you - it means that you're not tied to solving any one particular problem. You can pick what problem you're going to solve. So if you're a bacterium - this is a point that Chris Fields made a while back - where if you're a bacterium and you're in a concentration of sugar and you'd like to get more sugar, you have a couple of different options. You can learn to swim and solve this two dimensional or three dimensional movement problem, or you can change your metabolism and start to metabolize a completely different sugar. It doesn't matter, right? And so whereas we look at this and we say how do you evolve to solve a motion problem or whatever, life can simply switch to a different problem space and there's no requirement. So that's one thing to keep in mind is that by specifying individual problem spaces, we constrain in a way that evolution is not constrained by. Now specifically, how I think - what I think allows this is something that I call a multi-scale competency architecture. It's the fact that when we build, when we engineer - whether it be through software or hardware - when we engineer new agents, we typically have one, at best two levels of agency because we're working with dumb parts. So you're working with passive parts that you hope will come together to form something intelligent and that requires us as the engineer to build everything because we have to know how to assemble the parts in a particular way to get the outcome that we want. This is extremely constraining. In biology, biology is always working with active or agential material. So at every level, the molecular networks, the cells, the tissues, the organs, the organisms of swarms, every level has its own goals, has its own agendas, and they're all solving problems in various spaces. And the final outcome that you see is the result of coordination and competition within levels and between levels. Okay, so, so each level has its own goals. And so I think that the flexibility, the plasticity, the robustness that we see, comes from the fact that every level has its own goals. I'll give you a simple example from evolution of how that works. If you take a tadpole and you produce a tadpole, which we can do, where instead of the primary eyes in the head, there's an eye on the tail. And if you make a tadpole like that, they can see perfectly well out of those eyes. Because even though the primordial eye cells are sitting in a weird environment next to muscle instead of next to the brain, they'll form a perfectly good eye. They make this optic nerve. The optic nerve might connect to the spinal cord, the whole thing. The brain for millions of years expected visual data from a particular point in the head. But now there's information coming onto the spinal cord from some weird patch of tissue on its tail. No problem, it can learn to use that. And so that means that the whole thing is incredibly plastic. If you can count on your modules on your subroutines to get their jobs done - even when you make changes - that's very powerful. And why do they work? Because they can rely on their parts to get their job done when things have changed for them. Everybody is a - every piece of this - every module is a goal-seeking module. And what that means is it tries to get to a certain state despite perturbations and this is true at every level. So evolution always has to work with this kind of agential material. When evolution makes a change, it's not usually micromanaging what happens. It's usually having to control what the underlying agents are going to do. The individual cells are going to do things. So if you're an embryo, it's not just telling cells to make skin. It's actually suppressing them from doing other things they would otherwise want to do on their own. It's very much instructive. You are working in a reward space, not in a micromanagement space. Evolution has to work this way too, because all the parts always want to do things. If you don't coordinate them, they'll go off and do other stuff. And so much like with the xenobots. When we make these xenobots, all the cells do all the heavy lifting. They make the bots, we don't make the bots. All we've done is put them in the new environment. But what's amazing is when they reproduce, the cells themselves are doing exactly the same thing. All they're doing is collecting the other cells into a pile, but not micromanaging what happens next. They're taking advantage of the fact that these cells are also agents and that they will do their part and do what they need to do. So that working with agential materials, the fact that every level of this thing has its own agendas is what provides the robustness and flexibility, but also the open-endedness. Because when your parts have their own goals, in many ways, it becomes easier. But in other ways, it becomes harder to control what's going to happen next. Jan Feyereisl: Yeah, that to me makes perfect sense because one of the things that we tried to investigate for some time is essentially compared to the way that the current machine learning models are built. Rather than having some kind of end-to-end large monolithic system, focusing really on modular and collective systems. And when you're saying, if I understood correctly is that one thing is the modularity, the collective nature and the fact that each of those modules in the system has some agency or has some goal on their own, that essentially would be doing something no matter what, and that it would essentially be able to achieve particular states or particular goals despite some some small perturbation. So there's some level of robustness and when you put those things together, you're able to actually make the system much more interesting, much more robust, and much more open and generative in some sense compared to a single unit with a single goal. Michael Levin: Exactly, yeah. The high levels, the higher levels distort the option space for the lower levels and the lower levels are good at navigating those spaces - if anything, they avoid local minima or local maxima and things like that. But the idea is that all the higher levels can do - they never micromanage - all they can do is motivate, to reward or influence the lower levels towards specific goals, but that's all. And then the lower levels get their own thing done or not. Sometimes you get success and sometimes not. But the whole point of all this is to be able to scale goals and I think also scale stresses. So individual cells have very local cell scale goals, metabolic needs, pH, you know, things like that. Very, very local things. But once you have this modular TOTE loop*, where you test operate and exit, you have this homeostatic loop. If it's modular, you can plug in different things - what do you measure? What do you compare against? And what do you do? Think of a simple - like thermostats are simple - a simple homeostatic loop. If things are modular, you can plug in all kinds of things into that loop. And you can merge when cells are merged. When two cells are merged, when they've connected electrically, one of the things that means they do is when they take a measurement of what's going on, they take a bigger measurement. Instead of a single cell scale, now there's two cells. So if you have 100 cells, now they're taking a bigger measurement. So what that means is, along with the IQ rise of having multiple cells in the network, what that means is that you can now pursue much larger goals. Whereas individual cells pursue very, very humble sort of scaled or local goals, once you have a large network, that whole loop, the goals scale. And so now we can pursue things like let's make a finger instead of a single hand. No individual cell knows what a finger is, but the network can know. And the same thing about stress: individual cells are motivated in their activity by the stress that results from not being in their correct homeostatic state. So if you're hungry, you get some stress - metabolics are going down, you've got to eat - this is a single cell stress. But once you're in a network, and you can export that stress to other cells, you can share that information, then the other cells are motivated to act to reduce your stress because you're sharing your stress with them. So you're stressing them out, even though they don't have a local problem, but you have a global problem because your neighbor is now upset, and he's stressing you out. So stress is part of that glue that binds these collective agents together. Because by scaling the stress, you enlarge the cognitive space of the things that you are trying to implement. So think about any system that you look at, tell me what it's stressed by, and I could tell you what the cognitive level is. If you're stressed about local glucose concentrations, and that's it, well, you're probably a bacterium. If you're stressed about the global financial markets and what's going to happen 100 years from now to humanity, you're probably a human. And if you're stressed by how many other creatures have entered a space of some 100 meters, then you're some kind of mammal that's territorial. And if you're stressed by the fact that your eye is in the wrong location, you're probably an embryo trying to put its body together. So the scale of the things that you could possibly be stressed by is a great indicator of your overall intelligence. And that scales. These electrical networks help you scale your stresses, and thus they scale your goals. Jan Feyereisl: That's a really interesting viewpoint on that. If you would imagine that you would like to give some hints to engineers or machine learning researchers, people who want to essentially engineer an artificial life or some intelligent agents, the first question is, what would you suggest for them to focus on? And second, related to this notion of stress, how would you suggest stress to be essentially encoded in such artificial systems? Would it be through minimization or through some other metric or theme that you would suggest people to focus on? Michael Levin: Yeah, I'll give some thoughts. Obviously, I don't have a final answer. This is something that we're working on in my group, too. I think the most important piece of all of this is the multi-scale competency idea. The fact that every piece - I mean, you have to bottom out somewhere - but basically, every scale in every level in your system has to be a goal-directed agent. And it has to have a sense of this kind of homeostatic loop of what it is that it's trying to minimize and maximize. And then the problem boils down to how do we couple the goal-states, the stresses, and the measurements that each individual unit takes with the others in its lateral level, right? And so all the way down, you have to have this and so the bottom level, units have to compete for - if we take a cue from biology, I don't know if this is essential, if this is just how biology happens to do it - but the example from biology is the lowest level units have to compete for metabolic survival. So in your system, you have to have the ability of lower level subunits. If they're not doing the right things, they're not going to be rewarded by the next higher level. They're going to literally die. They're going to disappear. So they have to have skin in the game, they have their own local goals. But because their space is being bent by the system above, they have to be able to cooperate towards fulfilling some of the goals of the higher level. And of course, the higher level has the same problem with its higher level and so on. And so this is how I envision this multi-scale architecture working. And I think that that's the first step. Whether something else is going to be essential, I'm not sure, but I think this will be extremely powerful. Jan Feyereisl: So in some sense, not focusing on potentially one particular metric, but looking at the multiple scales all somehow jointly, or at the relationship between them, with all of the little details that you just talked about. Michael Levin: Yeah, that's what we're doing at this point. We're making some of these multi-scale kinds of simulations, looking at different ways that each level can reward and punish and incentivize and manipulate the levels below to propagate - in many ways, I love the field of machine learning because it has a really nice, there's a lot of concepts here that even though I think it's a very nascent stage, it's a lot of concepts that are very useful for biology. So this idea of credit assignment is really key, right? Because biology is amazing at credit assignments at every level. It seems to pick out exactly what I was doing when the good things happen, right? It seems to know. And so this idea of connecting subunits in ways where the lower levels are rewarded for doing the things that the higher level wants, but they're not micromanaged to do it, they're rewarded for it. So that's where all of the interesting search takes place - what are the policies for sharing that credit assignment, for scaling up the stresses, for connecting the subunits? That's where all the exciting progress is going to be, I think. Jan Feyereisl: This is again something that's very much close to our heart because currently, we're essentially struggling with the credit assignment problem in our collective system. So we're able to let it do something interesting. But when we actually somehow connect it to some external world and have it actually interact with the world in a way where it makes sense, and where the right parts of the collective actually get sufficient feedback, that's actually really tricky. So any kind of inspiration or ideas from biology, especially if it does well, that's always useful and helpful. And then another interesting question or topic is related to whether we're focusing on the right substrate of building our intelligent agents, whether we should be focusing a little bit more on not only on the ideas and algorithms that biology employs to do this amazing thing that it does, but also, as you have repeatedly mentioned, we essentially have this wonderful machinery and systems available to us. And if we look at them and communicate with them in the right way, they can do a lot of work for us. Should we focus a little bit more on actually interacting with the - or interfacing with biological systems and using their machinery and their ability to build things in order to actually create our artificial agents? What do you think about that? Michael Levin: I think certainly there's a lot of opportunity for really interesting engineering by instrumentalizing biology. So all of the technologies that are coming online - hybridization technologies, brain computer interfaces, which don't have to be brains, hybrids, Cyborg types of biological robotics - all of these things that really tightly integrate designed components, software components and living things at different scales, whether they be molecular computing or cellular computing. Yeah, I think lots and lots of great engineering is going to come from that. And I think it'll be very interesting. We're certainly involved in some of those things. I think long term, the important thing to keep our eye on is - and I think engineers do fine with this, I think biologists tend to go astray with it a little more - which is that when you do this, it isn't because the biology brings you some magic that is unattainable to engineers. It's just a temporary expediency that we use. I think that's important - there's a lot of biologists who have written things about how living things are fundamentally different from machines. And so they build up these binary categories, which I think is completely unsupportable given the chimerization. We can make any combination of living things and quote, unquote machines. And it's impossible to put these things in any kind of a binary category. So I think we have to realize there is nothing magical about living things. We are ultimately going to be able to someday reproduce in our engineered constructs, whatever it is that we see living things doing, because they are the result of natural processes, not magic. But that's going to take a long time and in the meantime, I think there's lots to be learned by including existing biological components with our engineering. And actually, one interesting thing is why is that even possible, right? Why is it even possible to take living cells and make them live on some sort of micro electrode array and make the whole thing play Pong or fly a flight simulator? Why is that even possible? Biology is incredibly interoperable. These cells have to survive in many different environments. We can make chimeras where human cells live next to drosophila cells and they do fine and we can instrumentize them, make them live next to nanomaterials and electrodes and in virtual worlds. Biology solves this kind of problem all the time. There's nothing new for these cells to live in some sort of weird bioreactor than it is to live inside an organism where they solve exactly the same problem - Who are my neighbors? How do I make them do good things for me? What are they making me do? Do I want to do these things? Or am I better off being a cancerous cell and going trying to go off on my own? These are trade offs that cells make all the time. And they're not at all surprised when we confront them with weird materials and whatever. So I think from that perspective, there's tons of good biology and engineering to be learned by making these kinds of hybrid constructs. But we shouldn't pretend that there's some sort of magic that we're going to be forever barred from if we don't use biology. Jan Feyereisl: Makes sense. Makes sense. Okay. One more question which is a little bit, maybe a little bit more distant from your work. But I was wondering your thoughts on this as well. And this relates to, essentially, if we talk about the different scales of cognition, intelligence, one of the things that we also find fascinating is cultural evolution and its cumulative nature. And in one of your work you mentioned the focus on the importance of gradualism, of the way that systems gradually kind of build up on each other. And whether you have any views on whether those things are connected, whether essentially, again, it's fundamentally due to the fact that there's some underlying mechanism, whether it's this multi-scale competency idea that essentially translates even to the level of culture and the way that essentially we teach our children and the environment around us and so on. Any thoughts? Michael Levin: I think the multiscale competency thing is really fundamental in that the first thing we need to do is realize that we are very bad at detecting agency. To be clear, we are great at detecting a very specific type of agency. So all of our - and this is most living things - all of our senses point outwards, and they measure things in the three dimensional world. So from the time that we're very small babies, we see objects moving around and we learn to assign - we learn the theory of mind, we learn to assign some agency. Okay, this is a bowling ball and all it's going to do is roll down the hill subject to the laws of physics. This thing is a mouse and it's going to do some very, very different things that I can't predict in the same way - I'm better off using rewards and motivations and other things if I want to manipulate what the animal does. All of that makes us good at detecting agency in the three dimensional world. But imagine, for example, if we had senses that looked inwards, let's say a biofeedback sense, that told you what your pancreas was doing at any point in the day. So if you had direct perception of all the things that were happening to your inner organs, and what they did, as a consequence, we would have no problem recognizing intelligence and navigating physiological space. You would say, wow, I see this thing learning for the last two weeks. Every day at lunch, I ate this particular thing and now it's anticipating, it's able to crank up certain enzymes because it knows that that same thing is coming. Here's how we dealt with a novel poison that was introduced into my system. So if we had these other training sets, we would be better at recognizing intelligence and weird spaces, these other sorts of physiological space, anatomical space, all these other kinds of spaces. So what I think we need to do is realize that because of that, I think we need to realize that we are only good with recognizing goal-directed systems in a very narrow range. Roughly medium size, like us roughly the same timescale like us, then we are good. In the same spaces, we are very bad at thinking about - we don't have any practice thinking about goal-directed systems, the scale of the whole evolutionary process. For example, a whole lineage. Do lineages have goals in the sense of not in a magical, religious sense, but in the sense of attractors in the space, where even though it's noisy, it's under a certain region of the possible space that it's going to try to get into, right? Or, if we could zoom into individual cell activity during embryogenesis, and you see all the noise, all the cells running around, we would never, in a million years be able to predict that, oh, yeah, this is always gonna make a fish embryo every single time. If you didn't already know what development was, you could never tell from looking at individual cell behaviors. So that means that both above us - meaning social levels of it, these kinds of structures - and below us - meaning your body organs and cells and other things - are tons of systems with diverse levels of agency, different goal-directed activities, different levels of IQ. We're blind to most of that. And so from this, the lessons that I take away from this is that, number one, you cannot tell what the agency or intelligence level of a particular system is, by philosophy. You can't sit there in your armchair and say, that can't be, it doesn't have a brain and thermostats don't have preferences and - you can make these philosophical pronouncements, but they mean nothing. What's important is experiment and asking what kind of model along that spectrum is the most effective at predicting and controlling what's going to happen. So the structures of which we are part, so let's say social structures - the Internet of Things, who knows what else - may well have certain degrees of goal directedness that we don't appreciate any more than our cells appreciate the goals that you and I have as emergent humans that come out of these cells. So these larger structures may be very primitive, and their goals may be almost non-existent or very, very minor, or they may be quite significant. We don't, we can't assume anything. We have to be open to the idea of experiments and I'm sure there's some sort of Gödelian limitation on what we can tell as far as what the systems - just like cells can't really conceive of our goals - I'm sure there are larger systems that we could be part of where we can't even begin to conceive what the goals are. But we have to be open to the fact that they may be there. And there may be mathematical tools to be developed to say, what type of system am I part of and can we say anything statistical about what kinds of goals it might have? And then maybe we will have some degree of agency to say, I want to be part of that, or actually, I defect. I don't want to be part of this. And of course, the higher system will see that in the same way that we see cancer, right? Yeah, that's great for you. But I don't want you to do that, I'd much rather you to sit there as a nice piece of skin. And when it's your time to fall off and die back, whatever, I'm the higher level, I don't care. So there will be this tension between the desires and the needs of us at one level and the levels above us. But we have to start developing tools to at least be able to imagine what these instructors might be. Jan Feyereisl: Interesting. So related to this, in some sense, and maybe a little bit more specific question - and I hope we still have a little bit more time - but one thing that is also interesting to us and it kind of relates to bioelectricity, but at the high level, if you can tell us about your viewpoint on how essentially groups are formed in a collective in such a way that they're beneficial to the collective as a whole. And somewhat related is in your work, this bioelectrical, which I view in some sense as some software or some actual program code that runs on the hardware of the body or the biological system that, for example, encodes a particular morphology. So where does that particular code come from? Michael Levin: Yeah. Let's talk about the code for a minute. I agree with you, and I speak of it this way all the time, that the bioelectric dynamics are a kind of software. And I think that they share some important properties with what we think of as software. Biologists get very, very upset often about that kind of terminology because they say, look, there's no step by step linear algorithm. Nobody sat down and wrote an algorithm, the cells aren't taking formal instructions off of a stack somewhere to execute - people say this a terrible analogy. But I think that it's important to first of all, generalize the idea that beyond the computers that we're familiar with - of course, living things aren't the kind of linear computers that we're used to - there are deeper aspects of reprogrammability and so on that are important. But the other thing about following an algorithm or being a code, I think, is that it's entirely in the eye of the beholder. In other words, I don't - I'm not sure there's any objective fact of the matter about whether something is a code, whether something is following an algorithm, right, versus just being a dynamical system, some sort of analog computer. All of that is in the eye of the beholder. We get fooled because we have examples that we made ourselves. And so if somebody says, hey, I wrote the algorithm for this thing and therefore, this is a real digital computer following an algorithm, and this thing over here, it just looks like physics to me, I don't see any algorithm, I got it from - It's evolved or whatever - and I think the problem there is that we are we are fooled into thinking that there's an objective answer to this. Because we're thinking of the very small class of things we made ourselves where we think that we know it's an algorithm. Imagine that we were given some sort of alien artifact, right? And let's say it's kind of squishy, and it's sort of biological but it's putting out some signals. So one person says, okay, all I see is physics and chemistry. It looks like a living thing. I don't think there's any algorithm here at all. And somebody else says, no, you don't understand, this is a - this plays alien chess, it's absolutely following an algorithm - if I interpret the outputs correctly, this is a lovely chess game that we can have. And the question is who's right? Because you don't have access to whoever made it, you don't know where it comes from. And whether or not something is a kind of software is something we paint onto it as a metaphor that helps us understand it. I don't think there's any answer to whether living things really are good codes, or machines or anything else. As an observer in regenerative medicine, as an observer trying to understand evolution, I think that some of these computational metaphors are extremely useful in pushing this forward, I think that's the best we're ever gonna be able to say in science - that these metaphors are helpful. And if you have a better metaphor, by all means, bring it out. And then we can abandon the older one, that's fine. But for now, these are great metaphors. So I think there is a code, where does it come from? So in part, it comes from - in every relationship like this of scientists to object, there are two players involved, right? There's the system itself, but then there's us. So part of this code comes from the observer, it comes from us with a particular understanding of what a mapping is, what a code is - so we bring that ourselves. Part of it comes from evolution and the fact that evolution figured out around the time of bacterial biofilms, that voltage gated current conductances - meaning ion channels that are voltage gated - they're transistors and once you have that, you can have anything, right, you can make anything move. But bacteria already have that. And so you can make these amazing brain-like electrical dynamics in bacterial biofilms that help them coordinate. So part of it comes from that. Part of it comes from the laws of physics and computation. They come from the fact that when you make a machine with a particular set of properties, it's a weird, platonic pythagorean kind of view, where I think you sort of manifest some laws that I don't know where these things hang - these things live wherever mathematical truths live, I don't know where that is - but you get to make logic gates and things that function like truth tables, and so on. If you can make a particular kind of machine and it doesn't matter what - is a basic functionalist idea, that doesn't matter what the machine is made of - it doesn't have to be biological but evolution certainly discovered that type of dynamic. And then you get to use these things. So the law, the code, rather, it has things like, well, if I make a particular electrical circuit, then I get to have memory, meaning that once the voltage is changed temporarily, I'm just going to keep that new voltage. You can make a flip flop out of that very easily. Or maybe the other way around - no, I'm extremely stable, you try to change my voltage, I'm going to snap right back as soon as you're done. Or something else that amplifies small differences, or something else that solves problems, like, how many cells are we - it's a big problem in cellular automata to figure out how to make a rule that counts cells - yeah, cells solve this all the time. They have electrical networks that can count and can say, okay, we are the right size. Now stop, stop whatever you're doing. So where do you know, where do those laws come from? I don't know. The same place that mathematics comes from, I guess, but it's very clear that evolution exploits all this stuff by making machines that take advantage of all that. Jan Feyereisl: I find this really fascinating. Because exactly the way that I view it is - or the way that I understand how you describe it in your research - is that yes, evolution gives you those parts; the genes are the hardware and on the hardware you can - or there is some software or some essentially code that exists or emerges during the lifetime of the biological system. And it has some memory, you can somehow augment it, change it, and you are able to essentially drive the hardware that seemed to have been most of the time or during evolution used for a particular software, you're able to actually change the software and within bounds, you're able to do a lot of different stuff. So that's super interesting to us. And, yeah, I was wondering, how is it possible and where does the actual original code come from. And I think you talked a lot about why it is robust in terms of all the multi scale-competency and many of the other things, so it's super, super interesting. Okay, thank you very much. I have a few lightning questions, if you don't mind. And those are specifically targeted to maybe getting people that are a little bit less versed in those topics and how we could kind of interest them in coming over and joining the workshop. So the first question is, if you can give some example of something that truly surprised you in your research. Michael Levin: Boy, it's hard to pick one. We've had many and that's why I love this field. I see surprising things all day long. But the most recent one is the xenobot replication, the ability that - the fact that these skin cells liberated from the rest of the animal within 48 hours get together to make a motile creature that figures out how to use these agential materials, these other cells as way to reproduce themselves the same way that we made the actual xenobots. Just seeing that, knowing that it's never happened to our knowledge in the history of life on earth, no other lineage does that. And here, to see this appearing in 48 hours in front of our eyes was just, just absolutely stunning to me. Jan Feyereisl: Thank you. And then the next question, what do you think researchers in machine learning and AI should really focus on when building intelligent systems or ultimately maybe trying to get to something like artificial general intelligence? And I think you mentioned the multi-scale competency idea, and so on. So if you would have to pick one and suggest what to start focusing on or where to go, where to investigate, what would you say? Michael Levin: Yeah, I think a really rich source of inspiration is life before brains. So look at all the fields of basal cognition, spend some time looking at protozoa, there's some great channels on YouTube and various live streams that are just a microscope set up over a Petri dish of pond water. And when you see those individual cells doing all the things that they do - there's no brain, there's no nervous system - and you just realize how competent each one of them is. And ask yourself, what would it take to get a few of them to cooperate together on a much larger goal, like building a body? Right? We're all bags of coupled amoebas, basically. And so that, to me, is the key to the whole thing. Jan Feyereisl: Thanks. And then last question, how important and relevant is machine learning for the outcomes of your work? If you would like to attract people to come and talk to you, to collaborate with you, what would you say in terms of the field of machine learning AI, how it relates to your work and your interests? Michael Levin: Yeah, it's hugely important. We have a few machine learning experts that work in my group. We need a lot more, both as a tool to use machine learning to analyze the data that we have, but also as an inspiration. I mean, we are all machines that learn right? So we are examples of machine learning. What other kinds of machines can learn, what are they learning? What are the concepts that we even need to begin to talk about these things beyond the material that they're made of. So yeah, absolutely. Experts in machine learning are extremely important to us. So yeah, we're open to conversations, for sure. Jan Feyereisl: Okay, thank you so much, Mike. It was really, really interesting. I've learned a lot of interesting things and concepts despite reading a lot about your work. There were many, many points that I kind of learned from it so I'm really grateful and happy for that. Michael Levin: Thank you so much. Jan Feyereisl: Thank you so much too and I guess we'll see each other at the workshop and I'm really looking forward to that and hoping that a lot of interesting people come and join and many more interesting outcomes will come out of it. Original interview: https://www. youtube. com/watch?v=87vRvmkJ9o8 Workshop: Cells to Societies: Collective Learning Across Scales Date: April 29 2022 Website: https://sites. google. com/view/collective-learning/home *Studies mentioned in interview: https://www. jstor. org/stable/184878 https://www. pnas. org/doi/full/10. 1073/pnas. 1910837117 https://www. pnas. org/doi/10. 1073/pnas. 2112672118 https://www. oxfordreference. com/view/10. 1093/oi/authority. 20110803105039783 For the latest from our blog, sign up for our newsletter. "
